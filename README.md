# Sydney Event Scraper with Gemini AI Backup

A web application that automatically scrapes events in Sydney, Australia and displays them beautifully on a web page. When traditional scraping fails (due to dynamic content, anti-bot protection, or page structure changes), it uses the Google Gemini API as an AI backup to extract or generate event details.

![Sydney Event Scraper Screenshot](/placeholder.svg?height=400&width=800)

## ğŸŒŸ Features

- **Smart Scraping Engine**: Uses Puppeteer to scrape Sydney events from popular sites
- **AI Backup**: Falls back to Google Gemini AI when traditional scraping fails
- **Beautiful UI**: Responsive design with filtering and search capabilities
- **Email Capture**: Collects user emails before redirecting to ticket pages
- **Admin Dashboard**: Monitor events, subscriptions, and trigger manual scrapes
- **Automated Updates**: Scheduled scraping to keep event data fresh

## ğŸ› ï¸ Tech Stack

- **Frontend**: Next.js, React, Tailwind CSS, shadcn/ui
- **Backend**: Next.js API Routes
- **Database**: MongoDB
- **Scraping**: Puppeteer
- **AI**: Google Gemini API (via @google/generative-ai)
- **Deployment**: Vercel

## ğŸ“‹ Prerequisites

- Node.js 18+ and npm
- MongoDB database
- Google Gemini API key

## ğŸš€ Getting Started

### Installation

1. Clone the repository:
   \`\`\`bash
   git clone https://github.com/yourusername/sydney-event-scraper.git
   cd sydney-event-scraper
   \`\`\`

2. Install dependencies:
   \`\`\`bash
   npm install
   \`\`\`

3. Set up environment variables:
   Create a `.env.local` file in the root directory with the following variables:
   \`\`\`
   MONGODB_URI=your_mongodb_connection_string
   MONGODB_DB=sydney_events
   GEMINI_API_KEY=your_gemini_api_key
   \`\`\`

4. Run the development server:
   \`\`\`bash
   npm run dev
   \`\`\`

5. Open [http://localhost:3000](http://localhost:3000) in your browser to see the application.

### Initial Setup

1. Trigger the initial scrape by visiting `/api/scrape` in your browser or using a tool like Postman.
2. Check the admin dashboard at `/admin` to see statistics about scraped events.

## ğŸ”§ Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `MONGODB_URI` | MongoDB connection string | Yes |
| `MONGODB_DB` | MongoDB database name (default: 'sydney_events') | Yes |
| `GEMINI_API_KEY` | Google Gemini API key | Yes |

## ğŸ“š Project Structure

\`\`\`
sydney-event-scraper/
â”œâ”€â”€ app/                    # Next.js app directory
â”‚   â”œâ”€â”€ api/                # API routes
â”‚   â”‚   â”œâ”€â”€ events/         # Events API
â”‚   â”‚   â”œâ”€â”€ email/          # Email subscription API
â”‚   â”‚   â”œâ”€â”€ scrape/         # Manual scrape trigger API
â”‚   â”‚   â””â”€â”€ subscriptions/  # Subscriptions API
â”‚   â”œâ”€â”€ admin/              # Admin dashboard
â”‚   â”œâ”€â”€ cron/               # Cron job endpoint
â”‚   â”œâ”€â”€ layout.tsx          # Root layout
â”‚   â””â”€â”€ page.tsx            # Main page
â”œâ”€â”€ components/             # React components
â”‚   â”œâ”€â”€ email-modal.tsx     # Email capture modal
â”‚   â”œâ”€â”€ event-card.tsx      # Event card component
â”‚   â”œâ”€â”€ event-filter.tsx    # Event filtering component
â”‚   â””â”€â”€ theme-provider.tsx  # Theme provider
â”œâ”€â”€ lib/                    # Utility functions
â”‚   â”œâ”€â”€ gemini.ts           # Gemini AI integration
â”‚   â”œâ”€â”€ mongodb.ts          # MongoDB connection
â”‚   â””â”€â”€ scraper.ts          # Web scraping logic
â”œâ”€â”€ types/                  # TypeScript type definitions
â”‚   â””â”€â”€ event.ts            # Event type definition
â””â”€â”€ public/                 # Static assets
\`\`\`

## ğŸ”„ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/events` | GET | Get all events |
| `/api/email` | POST | Store user email subscription |
| `/api/scrape` | GET | Manually trigger scraping |
| `/api/subscriptions` | GET | Get all email subscriptions |
| `/cron` | GET | Endpoint for scheduled scraping |

## ğŸ“± Usage

### Main Page

The main page displays all scraped events in a grid layout. Users can:
- Filter events by category, date, and location
- Click "Get Tickets" to be redirected to the event page (after providing their email)

### Admin Dashboard

The admin dashboard at `/admin` provides:
- Statistics about events and subscriptions
- A button to manually trigger scraping
- Tables of all events and subscriptions

### Setting Up Scheduled Scraping

To keep your event data fresh, set up a scheduled job to trigger the scraper:

1. **Using Vercel Cron Jobs**:
   - In your Vercel dashboard, go to Settings > Cron Jobs
   - Add a new cron job that hits your `/cron` endpoint
   - Set it to run daily (e.g., `0 0 * * *` for midnight every day)

2. **Alternative: External Cron Service**:
   - You can use services like Uptime Robot, Cronitor, or GitHub Actions
   - Set up a job to make a GET request to your `/cron` endpoint

## ğŸ”„ Automatic Event Updates

The system automatically updates events from source websites every 12 hours using Vercel's Cron Jobs. This ensures that the event data is always fresh and up-to-date.

### How it works:

1. A cron job runs every 12 hours, triggering the `/cron` endpoint
2. The scraper fetches the latest events from configured websites
3. New events are added to the database
4. Subscribers are notified about new events via email (if they've opted in)

### Email Notifications

When users click "Get Tickets" for an event, they're prompted to enter their email address. They can opt-in to receive notifications about:

- New events added to the system
- Updates to events they've shown interest in
- Similar events based on their preferences

Users can unsubscribe at any time by:
- Clicking the unsubscribe link in any email
- Visiting the `/unsubscribe` page on the website

## ğŸš¢ Deployment

This project is designed to be deployed on Vercel:

1. Push your code to a GitHub repository
2. Import the repository in Vercel
3. Set up the environment variables in the Vercel dashboard
4. Deploy the project

## ğŸ§© How It Works

### Scraping Process

1. The scraper attempts to extract events using traditional web scraping techniques
2. If traditional scraping fails, it falls back to the Gemini AI:
   - It sends the raw HTML or text content to the Gemini API
   - The API analyzes the content and extracts structured event data
   - The extracted data is stored in the database

### Email Capture

When a user clicks "Get Tickets":
1. They're prompted to enter their email
2. The email is stored in the database along with the event details
3. The user is redirected to the original event page

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgements

- [Next.js](https://nextjs.org/)
- [React](https://reactjs.org/)
- [Tailwind CSS](https://tailwindcss.com/)
- [shadcn/ui](https://ui.shadcn.com/)
- [MongoDB](https://www.mongodb.com/)
- [Puppeteer](https://pptr.dev/)
- [Google Generative AI](https://ai.google.dev/)
